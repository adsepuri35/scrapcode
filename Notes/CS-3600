Lecture 1:
AI - science of making machines that think like people, think rationally, act like people, act rationally.

Rational - maximally achieving pre-defined goals
Brief history of AI:
40s-50s - boolean circuit model of brain, Turing's "Computing Machinery and Intelligence"
50s-70s - early AI programs
70s-90s - statistical methods for speech recognition, systems industry booms
90s - statistical approaches, Deep Blue chess bot.
2000-2014 - Modern AI foundations, neural networks, AlphaGo, IBM Watson
2014-now - Deep Learning Revolution, imagenet accuracy

LLMs - transformer neural architecture, good at learning to emulate language, GPT-3 = 175 billion parameters. GPT-4 = 1.8 Trillion parameters


Lecture 2:
Agent - entity that perceives and acts.
Utility - numerical value assigned to a potential outcome or action. Represents how desirable an outcome is to an AI system.
Rational agent - selects actions to maximize expected utility

Agents perceives environment through sensors. The action space is determined and then actuators perform actions on the environment to influence it.

Reflex agents - choose action based on current perceptions. Don't consider future consequences.

Planning agents - make decisions based on consequences of actions. Have models of how world evolves.

Learning agents - involve feedback and learning goals

Search problem - consists of state space. Successor function (actions, cost). Start state and goal test.

Solution - set of actions that transforms start state to end state

Lecture 3:

Problem defined by - initial state, successor function (set of action pairs), goal test, path cost (additive)

Problem types:
Single state - deterministic and fully observable
Conformant - non-observable; solution is a sequence
Contingency - nondeterministic and/or partially observable. Percepts provide new info.
Exploration - unknown state space

State space graph - math representatioin of search problem
Goal test = set of goal nodes (maybe only one)
State space graph = all states are unique

Search tree:
- what if tree of plans and outcomes
- start state = root node
- can't build whole tree for most problems

state = representation of physical configuration
node = data structure part of tree
fringe - partial plans under consideration or set of all nodes at end of all visited paths

DFS: expand deepest node first. Fringe = LIFO stack. If m tiers in a tree, takes O(b^m) time
Not the most optimal because it finds the leftmost solution, regardless of depth or cost
Time = O(b^m)
Space = O(bm)
Not complete - fails in infinite depth states. spaces with loops

BFS - expand shallowest node first. Fringe = FIFO queue
Complete - Yes (b is finite)
Time = 1 + b + b^2 + ... + b^d = O(b^(d+1))
Space = O(b^(d+1))
Optimal = Yes (if cost = 1 per step); not optimal in general

Iterative Deepening - DFS space advantage with BFS time
Complete - Yes
Time = O(b^d)
Space = O(b^d)

Uniform Cost Search - expand a cheapest node first. Fringe = Priority queue. Priority = cumulative cost

Greedy Search - expand node that you think is closest to a goal state. Worst case = badly guided DFS.

Uniform-cost - orders by path cost or backwards cost, g(n)
Greedy - orders by goal proximity or forward cost, h(n)

A* Search: f(n) = g(n) + h(n)

Lecture 4: 

Admissable heuristic - heuristic function to estimate the distance from given node to goal node. It's considered admissable if it never overestimates true distance to the goal. Guarantee optimality

Heuristic is admissable if: 0 <= h(n) <= h*(n)
h*(n) is the true cost to the nearest goal

semi-lattice - partially ordered set with a least upper bound operation in context of heuristics (functions estimating distance from node to goal)

Main idea: estimated cost heuristics <= actual costs

Graph search - don't expand state/node twice

A*:
Uses backward and (estimated) forward costs
Optimal for admissable/consistent heuristics
Heuristic implementation is important.

Relaxed problems - modified versions of original problems (weaken constraints, simplify, gain understanding to problem structure)

Lecture 5:

Local search: use iterative improvement algos to improve a current state

Simulated annealing: global optimization algo
- allow bad moves occasionally
- high temp = more bad moves allowed. 
- Gradually reduce temp according to some schedule

Local Beam Search:
- Start with K copies of local search algorithm, initialized randomly
- During each iteration: Generate successors for all k states. Choose best K to be new current states.
- The searches communicate, so better than parallel local searches

Genetic Algos:
- Resample K individuals at each step (selection)
- Combine with crossover operation and mutation for variety

Lecture 6:

Game = Task environment with >1 agents
Want algorithms to find strategy to recommend moves from different states

Zero-Sum games - pure competition, opposite utils

General sum - independendent utilities, lot are possible (competition, cooperation, etc.)

Adverserial search - searching for optimal strats to maximize an individual's own payoff (minimizing opponent)

Minimax search - zero sum games. Consider opponents moves and make a move
Max = agent score you want to maximize
Min - opponent score you want to minimize
Same efficiencies as DFS

Alpha-Beta pruning - optimization technique for minimax search. reduce number of nodes to evaluate in game tree. Prune branches that won't affect final decision to improve efficiency.

Resource limits - can't search to leaves for most games -> depth-limited search. No guarantee of optimal play.

Evaluation functions - score non-terminals in depth-limited search. Assess desirability of a given state and provide a score.

Lecture 6 (again) - 

Game = task environment with > 1 agent
Policy = Solution for a player
Zero-Sum Games - agents with opposite utilities. pure competition - one maximizes while the other minimizes.
General-Sum Games - agents have independent utilities (measure of desirability of payoff from a certain outcome)
Adverserial search - agents consider the strategies of opponents.
Value of state - best achievable utility from a state
Minimax search - State space search tree, players alternate turns, compute each node's minimax value: (best utility achievable against a rational adversary). Uses evaluation function to assign scores for states. Chooses optimal path through recursive exploration and backtracking.
Multi-agent Utilities? - Terminal nodes have utility tuples where each player seeks to maximize their own component
Minimax efficiency - Time O(b^m), Space O(bm)
Alpha-beta pruning - no effect on actual minimax values. Alpha = Max's best option on path to root. Beta = Min's best option on path to root. Used to prune branches, subtrees
Resources Limits - Can't search to leaves in most games, so we use depth-limited search. No guarantee of optimal play
Evaluation function - score non-terminal states in depth-limited search. Weighted linear sum of certain features
Alpha-beta pruning - time complexity = O(b^(m/2)) in the best-case scenario