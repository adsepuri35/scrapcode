Lecture 1:
AI - science of making machines that think like people, think rationally, act like people, act rationally.

Rational - maximally achieving pre-defined goals
Brief history of AI:
40s-50s - boolean circuit model of brain, Turing's "Computing Machinery and Intelligence"
50s-70s - early AI programs
70s-90s - statistical methods for speech recognition, systems industry booms
90s - statistical approaches, Deep Blue chess bot.
2000-2014 - Modern AI foundations, neural networks, AlphaGo, IBM Watson
2014-now - Deep Learning Revolution, imagenet accuracy

LLMs - transformer neural architecture, good at learning to emulate language, GPT-3 = 175 billion parameters. GPT-4 = 1.8 Trillion parameters


Lecture 2:
Agent - entity that perceives and acts.
Utility - numerical value assigned to a potential outcome or action. Represents how desirable an outcome is to an AI system.
Rational agent - selects actions to maximize expected utility

Agents perceives environment through sensors. The action space is determined and then actuators perform actions on the environment to influence it.

Reflex agents - choose action based on current perceptions. Don't consider future consequences.

Planning agents - make decisions based on consequences of actions. Have models of how world evolves.

Learning agents - involve feedback and learning goals

Search problem - consists of state space. Successor function (actions, cost). Start state and goal test.

Solution - set of actions that transforms start state to end state

Lecture 3:

Problem defined by - initial state, successor function (set of action pairs), goal test, path cost (additive)

Problem types:
Single state - deterministic and fully observable
Conformant - non-observable; solution is a sequence
Contingency - nondeterministic and/or partially observable. Percepts provide new info.
Exploration - unknown state space

State space graph - math representatioin of search problem
Goal test = set of goal nodes (maybe only one)
State space graph = all states are unique

Search tree:
- what if tree of plans and outcomes
- start state = root node
- can't build whole tree for most problems

state = representation of physical configuration
node = data structure part of tree
fringe - partial plans under consideration or set of all nodes at end of all visited paths

DFS: expand deepest node first. Fringe = LIFO stack. If m tiers in a tree, takes O(b^m) time
Not the most optimal because it finds the leftmost solution, regardless of depth or cost
Time = O(b^m)
Space = O(bm)
Not complete - fails in infinite depth states. spaces with loops

BFS - expand shallowest node first. Fringe = FIFO queue
Complete - Yes (b is finite)
Time = 1 + b + b^2 + ... + b^d = O(b^(d+1))
Space = O(b^(d+1))
Optimal = Yes (if cost = 1 per step); not optimal in general

Iterative Deepening - DFS space advantage with BFS time
Complete - Yes
Time = O(b^d)
Space = O(b^d)

Uniform Cost Search - expand a cheapest node first. Fringe = Priority queue. Priority = cumulative cost

Greedy Search - expand node that you think is closest to a goal state. Worst case = badly guided DFS.

Uniform-cost - orders by path cost or backwards cost, g(n)
Greedy - orders by goal proximity or forward cost, h(n)

A* Search: f(n) = g(n) + h(n)

Lecture 4: 

Admissable heuristic - heuristic function to estimate the distance from given node to goal node. It's considered admissable if it never overestimates true distance to the goal. Guarantee optimality

Heuristic is admissable if: 0 <= h(n) <= h*(n)
h*(n) is the true cost to the nearest goal

semi-lattice - partially ordered set with a least upper bound operation in context of heuristics (functions estimating distance from node to goal)

Main idea: estimated cost heuristics <= actual costs

Graph search - don't expand state/node twice

A*:
Uses backward and (estimated) forward costs
Optimal for admissable/consistent heuristics
Heuristic implementation is important.

Relaxed problems - modified versions of original problems (weaken constraints, simplify, gain understanding to problem structure)

Lecture 5:

Local search: use iterative improvement algos to improve a current state

Simulated annealing: global optimization algo
- allow bad moves occasionally
- high temp = more bad moves allowed. 
- Gradually reduce temp according to some schedule

Local Beam Search:
- Start with K copies of local search algorithm, initialized randomly
- During each iteration: Generate successors for all k states. Choose best K to be new current states.
- The searches communicate, so better than parallel local searches

Genetic Algos:
- Resample K individuals at each step (selection)
- Combine with crossover operation and mutation for variety

Lecture 6:

Game = Task environment with >1 agents
Want algorithms to find strategy to recommend moves from different states

Zero-Sum games - pure competition, opposite utils

General sum - independendent utilities, lot are possible (competition, cooperation, etc.)

Adverserial search - searching for optimal strats to maximize an individual's own payoff (minimizing opponent)

Minimax search - zero sum games. Consider opponents moves and make a move
Max = agent score you want to maximize
Min - opponent score you want to minimize
Same efficiencies as DFS

Alpha-Beta pruning - optimization technique for minimax search. reduce number of nodes to evaluate in game tree. Prune branches that won't affect final decision to improve efficiency.

Resource limits - can't search to leaves for most games -> depth-limited search. No guarantee of optimal play.

Evaluation functions - score non-terminals in depth-limited search. Assess desirability of a given state and provide a score.

Lecture 5:
Local Search Algorithms:
- Path is irrelevant. Goal is solution.
- Find configuration satisfying constraints.
- Iterative improvement algos

Simulated annealing:
- allow bad moves occasionally.
- High temp = more bad moves allowed

Local beam search:
- Copies of local search algos, randomly intialized randomly

Lecture 6 (again) - 

Game = task environment with > 1 agent
Policy = Solution for a player
Zero-Sum Games - agents with opposite utilities. pure competition - one maximizes while the other minimizes.
General-Sum Games - agents have independent utilities (measure of desirability of payoff from a certain outcome)
Adverserial search - agents consider the strategies of opponents.
Value of state - best achievable utility from a state
Minimax search - State space search tree, players alternate turns, compute each node's minimax value: (best utility achievable against a rational adversary). Uses evaluation function to assign scores for states. Chooses optimal path through recursive exploration and backtracking.
Multi-agent Utilities? - Terminal nodes have utility tuples where each player seeks to maximize their own component
Minimax efficiency - Time O(b^m), Space O(bm)
Alpha-beta pruning - no effect on actual minimax values. Alpha = Max's best option on path to root. Beta = Min's best option on path to root. Used to prune branches, subtrees
Resources Limits - Can't search to leaves in most games, so we use depth-limited search. No guarantee of optimal play
Evaluation function - score non-terminal states in depth-limited search. Weighted linear sum of certain features
Alpha-beta pruning - time complexity = O(b^(m/2)) in the best-case scenario

Lecture 7:
Expectimax Search - variant of Minimax Search, uses probablility and expectation to handle uncertainty.
Values reflect average case outcomes (not worst-case)
Max nodes like minimax search but chance nodes are like min nodes with an uncertain outcome. So we calculate expected utility (take weighted avg of children)

Minimax - better for perfect information games (both players have complete knowledge of game state). Assume opponents play optimally. Ex. Chess, Tic-Tac-Toe

Expectimax - probabilistic environments. Take EV of possible outcomes at chance nodes instead of assuming optimal opponent play. Ex. Poker, 2048. More computationally expensive. Doesn't handle opponent

Expectiminimax: 3 types of nodes. Max nodes, min nodes, chance nodes (random events) that compute EV by averaging outcomes with probabilities.
Ex. backgammon (dice rolls are random), Poker (random card draws) Best of both worlds.

Roullouts - simulate games from the current position to a terminal state using a policty (often random moves)

MCTS:
Evaluation by rollouts - play multiple games to termination from state s and count wins + losses.
Selective search - explore parts of tree that improve decision at the root, regardless of depth.
True value of position = fraction of wins

UCB - Upper Confidence Bound Algorithm - balance exploration and exploitation.
Formula: Total reward/number of times action selected + Exploration Constant C (Higher C = more exploration)

Lecture 8:
MCTS - Repeat until out of time.
- Selection: recursively apply UCB to choose path down to leaf node n
- Expansion: add new child c to n
- Simulation: run rollout from c
- Update U and N counts from c back to root
N(n) = # of rollouts from node n
U(n) = total utility of rollouts (ex. # of wins)

Simulated annealing:
Start with a high temperature (more exploration) and gradually decrease temperature, making the algo more selective. 

Local Beam Search:
Heuristic algo that combines elements of local search and beam search. LBS explores solution space by generating new solution through neighborhood function. Selects k most promising area at each iteration. Eventually converges to a good solution


Quiz 3 Studying:

Lecture 7:
alpha = MAX's best option on path to root
beta = MIN's best option on path to root

Evalutation function:
- ideally returns actual minimax value of position
- in practice is the weighted linear sum of features

Expectimax - compute average score under optimal play. We have chance nodes now that are like min nodes with uncertain outcomes. Suited for probabilistic outcomes. Uses expectation-based evaluation function.

Minimax uses a simple evaluation function and searches the game tree depth-first.

Expectiminimax - combination of the 2.

MCTS:
Rollouts = play multiple games to termination from a state and count wins and losses. 
Selective search = explore parts of the tree what will help improve the decision at the root, regardless of depth.

UCB (Upper Confidence Bound):
N(n) = number of rollouts from node n
U(n) = total utility of rollouts (# of wins)

Lecture 8:
Propositional Logic - deals with simple statements (propositions) than be either true or false. Lacks quantifiers.

First-Order Logic - uses predicates/relations to describe properties and relationships. Includes quantifiers and allows functions.

Entailment: a |= b. Means that a entails b or that every world where a is true, b is true.

Proof - demonstration of entailment between a and b.
Sound algorithm - all conclusions drawn from this algo are guaranteed to be true.
Complete algorithm - can prove every statement that is logically entailed.

Implication - syntactic relationship between two statements, where one statement logically leads to another. Ex. α ⇒ β

Successor-state axiom - logical statement that specifies the conditions under which a property or relation changes its truth value from one state to another

Reasoning Tasks:
- localization (where am I?)
- Mapping
- simultaneous localization and mapping
- Planning (what action sequences is guaranteed to reach the goal?)

Lecture 9:
Utilities - functions that describe an agent's preferences.

A rational agent should choose the action that maximizes its expected utility.

Preference: A > B
Indifference: A ~ B

MEU Principle:
- Choose the action that maximizes expected utility
- Rationality does not require manipulating utilities and probabilities

Utilities map states to real numbers.

Decision Networks:
Nodes for utility and actions
Chance nodes = ovals
Actions = rectangles
Utility node = diamond 

Notation:
EU(leave) = Expected utility of taking action leave
MEU(0) = MEU, given no info
EU(leave | bad) = EU of choosing leave given the forecast is bad
MEU(F = bad) = MEU given you know the forecast is bad

Value of Perfect Information (VPI)
MEU(e) = MEU given evidence E = e
VPI(E' | e) = expected gain in utility for knowing the value of E', given I know the value of e so far

Value of information = expected improvement in decision quality.

Quiz 4:

Lecture 9:
Utilities - functions that describe agent preferences

Maximum Expected Utility - rational agent would choose an action that maximizes expected utility.

Magnitudes can help differentiate between more desirable and less desirable outcomes.

Decision Networks - help make optimal decisions when outcomes are uncertain. Use MEU

VPI - quantifies how much a decision maker would be willing to pay to obtain complete and accurate info about uncertain event before making a decision.

VPI = EV (w/ perfect info) - EV (w/o perfect info)

Lecture 10-12:

MDPs:
Consist of set of states, set of actions, transition function, reward function. Model decision-making in situations that are partly random and partly under control of a decision-maker.

Goal: find a policy that maximizes the expected cumulative reward over time.

To solve an MDP, we need to find the optimal policy that maximizes the expected cumulative reward.

Value Iteration:
Iteratively computes the value function V(s). Represents the expected cumulative reward starting from state s and following the optimal policy. Uses the Bellman equation to update the value function. Once the value function converges, the optimal policy can be derived by choosing actions that maximize the EV.

Policy Iteration:
Alternates between policy evaluation (computing the value function for a given policy) and policy improvement (updating the policy to maximize the value function). This continues until the policy converges to the optimal policy.

Discount Factor: Number between 0 and 1 (gamma). Represents preference of immediate rewards over future rewards. Used to compute present value of future rewards in the cumulative reward calculation.
0 = only immediate rewards matter, future ignored
1 = future rewards valued same as immediate
0 - 1 = future rewards discounted exponentially over time

Policy extraction - deriving the optimal policy from a given value function V(s). Agents selects the action that leads to the highest EV. 

Certainty Equivalence:
Guaranteed amount of money that an individual considers equally desirable as a risky gamble.