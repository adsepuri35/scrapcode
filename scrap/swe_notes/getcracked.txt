OS:
2 tradeoffs scheduler has to decide between - response time & turnaround time. 
turnaround time = amount of time elapsed from submission to completion
response time = avg time elapsed from submission until first response produced. 
Schedulers should prioritize response time first.

Primary purpose of OS - virtualize machine's physical hardware. OS makes system easier to use and more efficient.

Time sharing/slicing - illusion that multiple virtual CPUs exist through a running process, stopping it, running another, stopping it..

Standard process states - ready, blocked, running

Process list - kernel data structure that keeps track of active processes. Data structure of PCBs. Can be used for scheduling, signaling, resource tracking, and process termination. 

Interrupting an interrupt - might disable interrupts (not for too long). What the OS doesn't do is abort the current interrupt and handle the new one. This could lead to data corruption or deadlocks.

Nice - (-20 to +19, default = 0) parameter for a process to indicate priority. Negative = higher priority for schedulers and Positive = lower priority for schedulers. Think about it like this: If you're too nice, you don't get much scheduling attention.

Completely Fair Scheduler (CFS) - default process scheduler for Linux. Fair CPU time is distributed based on process weights. CFS doesn't use fixed time slices. It uses a Red-black tree to track process based on virtual runtime. Proportional share based on priority.

vruntime - how much CPU time a process has used.

RB-tree - time ordered and processes are stored by vruntime. The leftmost node has the smallest vruntime and is chosen for execution. So the most CPU-starved task runs first.

RB-tree = self-balancing binary tree (fast storage). Nodes can be either red or black. Root always black. Red nodes can't have red children. For black nodes, every path from a node to its leaves has the same number of black nodes. All null nodes (past leaves) are black. Guarantee the height is O(log n) as opposed to BST.

Why is Compaction so compute intensive?:
Compaction is rearranging allocated memory blocks to reduce fragmentation. It requires copying live data from one location to another, so if allocations are scattered, theres a ton of overhead and copying power required. Pointers must be updated. Running threads must be stopped so no invalid memory accesses occur. Caches and TLB must be updated.

magic - number/signature to validate a memory chunk. Stored in metadata of free blocks. If magic is overwritten, the allocator knows the chunk was corrupted. It can also help distinguish between allocated vs free blocks.

Allocation cost - computational cost required to reserve memory for variables or structures in a program. Global static variables are not allocated since memory is reserved at complile time. Stack allocated variables have minimual cost (adjusting stack pointer). Dynamically allocated stuff have high cost (heap management, fragmentation, sys calls)

Executable files can be broken down into different sections:
.text - stores executable code
.data - initialized data (global/static variables)
.bss - global/static variables not initialized (only size info stored in executable)

PFN - identify page in RAM
Page tables map VPNs to PFNs

So when translating addresses, walking the page tables can be expensive, this is where TLBs come into play.

Inverted page tables - instead of per process, one global table for the whole system. Map PFNs to VPNs and PIDs. Scales with physical memory instead of virtual memory so they are generally more compact. Built on a hash table instead of walk. However, there are no TLB optimizations and its expensive to resolve hash collisions.

Clock replacement algorithm - page replacement strat. Approximates LRU with less overhead. Treats the set of pages in memory as a circular list. Uses a reference bit per page to track usage. Scans pages like a clock hand and gives pages a second chance before eviction.

Zombie thread - thread that completed execution by still occupies system resources because its parent thread or process was not properly cleaned up by calling a synchronization function (pthread_join()).

Compare and swap (CAS) - atomic operation to implement synchronization without locks. Ensure shared variable is only updated if its current value is the expected value, preventing race conditions.

Hand-Over-Hand Locking / Lock Coupling - concurrency strat used in linked list data structures to allow multiple theads to traverse and modify the structure while maintaining thread safety. 
Locks are acquired one node at a time, instead of locking the whole structure. Thread can hold max 2 locks: curent node and next node locks

/dev/urandom - get random num quickly in Linux, uses internal pool to produce more bits
/dev/random - higher quality randomness

entropy pool - collection of random bits collected from system events - keyboard timings, mouse moving, etc. Used as a source of randomness for the kernel.

Standard file descriptors:
0 - stdin
1 - stdout
2 - stderr

File descriptors - small integers in the OS that are used to refer to open files and standard I/O streams. When a program starts, its automatically given the 3 standard file descriptors.

Paging Heirarchy:
Level 1 - Page Table, PT - Point to physical pages
Level 2 - Page Directory, PD - Points to Page Tables
Level 3 - Page Directory Pointer Tables, PDPT - Points to Page Directories
Leve 4 - Page Map Level 4, PML4 - Top level table pointing to PDPTs

Each level uses 9 bits of the virtual address to index into a page table.

In C++, int[] are stack allocated and have a predetermined size.
std::vector are resizable and automatically handles memory management. They are also heap-allocated.

To study:
Question - PML4
Question - PML?


Computer Architecture

Cache misses:
Compulsory - item was never in Cache
Capacity - item was in cache but not enough space so it was forced out
Conflict - item was in cache but was not associative enough so it was forced out

ISA - interface between software and hardware. 
Main components of ISA: Instruction Set, Registers, Data Types 
Categories of ISA instructions - Arithmetic/Math, Data Transfer (Load, store), Logical (AND, OR, XOR, etc.)

Associativity - tries to answer which locations of RAM memory (addresses) map to which block frames of the CPU's caches. Associativity balances trade-offs between speed, hit rate, and hardware complexity

Common forms of CPU cache associativity - direct, 2, 4, 8-way

Cache lines are typically 32-128 bytes

Cache affinity - the tendency of a process of thread to build up state and reuse the state in the cache

Compilers often align data by padding non-word aligned types with empty bytes for hardware efficiency purposes. Align makes it less prone to error.

Register-relative addressing = base address + offset
Base address is stored in a register and offset specidies the distance in bytes from that address. Because of this, programmers can code without knowing exact memory locations.

Steps of Fetch-execute loop - Fetch, decode, execute

Branch instructions most effect the program counter (PC)

Pipelining doesnt reduce execution time of instructions but increases the number of instructions completed per time (throughput).

Single cycle processors waste hardware resourcs because other parts of the processors just sit idle.


To Study:
Sharing is caring.
L1 Cache Split
NUMA NUMA NUMA
TLB Switch
How much parallelism?
RAM to Cache Memory Transport
Tag, you're it.
How big is it?
It's massive
Cache Coherence - MESI?
Warm and Toasty
Pipeline Operation Basics
RISC Pipeline Structure
Processor Architectural Divisions
Pipeline Performance Calculation
Clock Cycle and Pipeline Depth
Cache Inclusion
Nosy Cache
What 'E'?


Concurrency: 

yield() - thread provides a hint for the scheduler to put this thread to sleep and reschedule it for later.

Spurious wake - thread waiting on a condition variable is awakened without the condition it was waiting for being met. Interrupts or other low-level events can cause it to happen unexpectedly.

Race condition - when the outcome depends on the relative timing of operations between threads

join() - blocks the calling thread until the other thread completes execution. It is used for synchronization.


To Study:
2 Types of Parallelism
How does it mutex?
Same line, different time.
Mutex Management with lock_guard
Thread Ownership Transfer
Optimal Thread Count
What's suspect?
Safe Data Sharing Techniques
Thread Granularity Control
Mutex Protection Bypass
Memory Ordering Guarantees
Something is wrong.
Locking Granularity Tradeoffs
Exception Safety in Concurrent Code
Lock Free vs Wait Free - PRIORITY
ABA Problem in Lock-free Code
Lock-free Memory Management
What pointer?
Amdahl's Law Application
Dynamic Workload Distribution
Cache Coherence Optimization
Thread Pool Benefits
Work Stealing Implementation
Cross-thread Exception Handling
Parallel Algorithm Pitfalls



Networking:


To Study:
